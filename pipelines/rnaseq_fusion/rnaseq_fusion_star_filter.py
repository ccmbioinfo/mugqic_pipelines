#!/usr/bin/env python

################################################################################
# Copyright (C) 2014, 2015 GenAP, McGill University and Genome Quebec Innovation Centre
#
# This file is part of MUGQIC Pipelines.
#
# MUGQIC Pipelines is free software: you can redistribute it and/or modify
# it under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# MUGQIC Pipelines is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with MUGQIC Pipelines.  If not, see <http://www.gnu.org/licenses/>.
################################################################################

# Python Standard Modules
import argparse
import collections
import logging
import os
import re
import sys

# Append mugqic_pipelines directory to Python library path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))))

# MUGQIC Modules
from core.config import *
from core.job import *
from core.pipeline import *
from bfx.design import *
from bfx.readset import *

from bfx import bedtools
from bfx import cufflinks
from bfx import differential_expression
from bfx import gq_seq_utils
from bfx import htseq
from bfx import metrics
from bfx import picard
from bfx import samtools
from bfx import star
from bfx import bvatools
from bfx import rmarkdown
from pipelines import common

from bfx import defuse
from bfx import fusionmap
from bfx import tophat2
from bfx import integrate
from bfx import ericscript
from bfx import star_filter
from bfx import gunzip
from bfx import merge_fastq
from bfx import cff_convertion
from bfx import check_dna_support_before_next_exon
from bfx import merge_and_reannotate_cff_fusion

import utils

log = logging.getLogger(__name__)

class RnaFusion(common.Illumina):
	"""
	RNA-Seq Pipeline
	================

	The standard MUGQIC RNA-Seq pipeline is based on the use of the [STAR aligner](https://code.google.com/p/rna-star/)
	to align reads to the reference genome. These alignments are used during
	downstream analysis to determine genes and transcripts differential expression. The
	[Cufflinks](http://cufflinks.cbcb.umd.edu/) suite is used for the transcript analysis whereas
	[DESeq](http://bioconductor.org/packages/release/bioc/html/DESeq.html) and
	[edgeR](http://bioconductor.org/packages/release/bioc/html/edgeR.html) are used for the gene analysis.

	The RNAseq pipeline requires to provide a design file which will be used to define group comparison
	in the differential analyses. The design file format is described
	[here](https://bitbucket.org/mugqic/mugqic_pipelines/src#markdown-header-design-file)

	The differential gene analysis is followed by a Gene Ontology (GO) enrichment analysis.
	This analysis use the [goseq approach](http://bioconductor.org/packages/release/bioc/html/goseq.html).
	The goseq is based on the use of non-native GO terms (see details in the section 5 of
	[the corresponding vignette](http://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf).

	Finally, a summary html report is automatically generated by the pipeline at the end of the analysis.
	This report contains description
	of the sequencing experiment as well as a detailed presentation of the pipeline steps and results.
	Various Quality Control (QC) summary statistics are included in the report and additional QC analysis
	is accessible for download directly through the report. The report includes also the main references
	of the software tools and methods used during the analysis, together with the full list of parameters
	that have been passed to the pipeline main script.

	An example of the RNA-Seq report for an analysis on Public Corriel CEPH B-cell is available for illustration
	purpose only: [RNA-Seq report](http://gqinnovationcenter.com/services/bioinformatics/tools/rnaReport/index.html).

	[Here](https://bitbucket.org/mugqic/mugqic_pipelines/downloads/MUGQIC_Bioinfo_RNA-Seq.pptx) is more
	information about the RNA-Seq pipeline that you may find interesting.
	"""

	def __init__(self):
		# Add pipeline specific arguments
		self.argparser.add_argument("--sampleinfo", help="sample info file", type=file)
		self.argparser.add_argument("--dnabam", help="DNA bam list", type=file)
		super(RnaFusion, self).__init__()

	def star(self):
		"""
		The filtered reads are aligned to a reference genome. The alignment is done per readset of sequencing
		using the [STAR](https://code.google.com/p/rna-star/) software. It generates a Binary Alignment Map file (.bam).

		This step takes as input files:

		1. Trimmed FASTQ files if available
		2. Else, FASTQ files from the readset file if available
		3. Else, FASTQ output files from previous picard_sam_to_fastq conversion of BAM files
		"""

		jobs = []
		project_index_directory = "reference.Merged"
		project_junction_file = os.path.join("alignment_1stPass", "AllSamples.SJ.out.tab")
		individual_junction_list=[]
		######
		#pass 1 -alignment
		for readset in self.readsets:
			trim_file_prefix = os.path.join("trim", readset.sample.name, readset.name + ".trim.")
			alignment_1stPass_directory = os.path.join("alignment_1stPass", readset.sample.name, readset.name)
			individual_junction_list.append(os.path.join(alignment_1stPass_directory,"SJ.out.tab"))

			if readset.run_type == "PAIRED_END":
				candidate_input_files = [[trim_file_prefix + "pair1.fastq.gz", trim_file_prefix + "pair2.fastq.gz"]]
				if readset.fastq1 and readset.fastq2:
					candidate_input_files.append([readset.fastq1, readset.fastq2])
				if readset.bam:
					candidate_input_files.append([re.sub("\.bam$", ".pair1.fastq.gz", readset.bam), re.sub("\.bam$", ".pair2.fastq.gz", readset.bam)])
				[fastq1, fastq2] = self.select_input_files(candidate_input_files)
			elif readset.run_type == "SINGLE_END":
				candidate_input_files = [[trim_file_prefix + "single.fastq.gz"]]
				if readset.fastq1:
					candidate_input_files.append([readset.fastq1])
				if readset.bam:
					candidate_input_files.append([re.sub("\.bam$", ".single.fastq.gz", readset.bam)])
				[fastq1] = self.select_input_files(candidate_input_files)
				fastq2 = None
			else:
				raise Exception("Error: run type \"" + readset.run_type +
				"\" is invalid for readset \"" + readset.name + "\" (should be PAIRED_END or SINGLE_END)!")

			rg_platform = config.param('star_align', 'platform', required=False)
			rg_center = config.param('star_align', 'sequencing_center', required=False)

			job = star.align(
				reads1=fastq1,
				reads2=fastq2,
				output_directory=alignment_1stPass_directory,
				genome_index_folder=None,
				rg_id=readset.name,
				rg_sample=readset.sample.name,
				rg_library=readset.library if readset.library else "",
				rg_platform_unit=readset.run + "_" + readset.lane if readset.run and readset.lane else "",
				rg_platform=rg_platform if rg_platform else "",
				rg_center=rg_center if rg_center else ""
			)
			job.name = "star_align.1." + readset.name
			jobs.append(job)
		
		######
		jobs.append(concat_jobs([
		#pass 1 - contatenate junction
		star.concatenate_junction(
			input_junction_files_list=individual_junction_list,
			output_junction_file=project_junction_file
		),
		#pass 1 - genome indexing
		star.index(
			genome_index_folder=project_index_directory,
			junction_file=project_junction_file
		)], name = "star_index.AllSamples"))

		######
		#Pass 2 - alignment
		for readset in self.readsets:
			trim_file_prefix = os.path.join("trim", readset.sample.name, readset.name + ".trim.")
			alignment_2ndPass_directory = os.path.join("alignment", readset.sample.name, readset.name)

			if readset.run_type == "PAIRED_END":
				candidate_input_files = [[trim_file_prefix + "pair1.fastq.gz", trim_file_prefix + "pair2.fastq.gz"]]
				if readset.fastq1 and readset.fastq2:
					candidate_input_files.append([readset.fastq1, readset.fastq2])
				if readset.bam:
					candidate_input_files.append([re.sub("\.bam$", ".pair1.fastq.gz", readset.bam), re.sub("\.bam$", ".pair2.fastq.gz", readset.bam)])
				[fastq1, fastq2] = self.select_input_files(candidate_input_files)
			elif readset.run_type == "SINGLE_END":
				candidate_input_files = [[trim_file_prefix + "single.fastq.gz"]]
				if readset.fastq1:
					candidate_input_files.append([readset.fastq1])
				if readset.bam:
					candidate_input_files.append([re.sub("\.bam$", ".single.fastq.gz", readset.bam)])
				[fastq1] = self.select_input_files(candidate_input_files)
				fastq2 = None
			else:
				raise Exception("Error: run type \"" + readset.run_type +
				"\" is invalid for readset \"" + readset.name + "\" (should be PAIRED_END or SINGLE_END)!")

			rg_platform = config.param('star_align', 'platform', required=False)
			rg_center = config.param('star_align', 'sequencing_center', required=False)

			job = star.align(
				reads1=fastq1,
				reads2=fastq2,
				output_directory=alignment_2ndPass_directory,
				genome_index_folder=project_index_directory,
				rg_id=readset.name,
				rg_sample=readset.sample.name,
				rg_library=readset.library if readset.library else "",
				rg_platform_unit=readset.run + "_" + readset.lane if readset.run and readset.lane else "",
				rg_platform=rg_platform if rg_platform else "",
				rg_center=rg_center if rg_center else "",
				create_wiggle_track=True,
				search_chimeres=True,
				cuff_follow=True,
				sort_bam=True
			)
			job.input_files.append(os.path.join(project_index_directory, "SAindex"))
 
			# If this readset is unique for this sample, further BAM merging is not necessary.
			# Thus, create a sample BAM symlink to the readset BAM.
			# remove older symlink before otherwise it raise an error if the link already exist (in case of redo)
			if len(readset.sample.readsets) == 1:
				readset_bam = os.path.join(alignment_2ndPass_directory, "Aligned.sortedByCoord.out.bam")
				sample_bam = os.path.join("alignment", readset.sample.name ,readset.sample.name + ".sorted.bam")
				job = concat_jobs([
					job,
					Job([readset_bam], [sample_bam], command="ln -s -f " + os.path.relpath(readset_bam, os.path.dirname(sample_bam)) + " " + sample_bam, removable_files=[sample_bam])])

			job.name = "star_align.2." + readset.name
			jobs.append(job)

		report_file = os.path.join("report", "RnaSeq.star.md")
		jobs.append(
			Job(
				[os.path.join("alignment", readset.sample.name, readset.name, "Aligned.sortedByCoord.out.bam") for readset in self.readsets],
				[report_file],
				[['star', 'module_pandoc']],
				command="""\
mkdir -p report && \\
pandoc --to=markdown \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable scientific_name="{scientific_name}" \\
  --variable assembly="{assembly}" \\
  {report_template_dir}/{basename_report_file} \\
  > {report_file}""".format(
					scientific_name=config.param('star', 'scientific_name'),
					assembly=config.param('star', 'assembly'),
					report_template_dir=self.report_template_dir,
					basename_report_file=os.path.basename(report_file),
					report_file=report_file
				),
				report_files=[report_file],
				name="star_report")
		)

		return jobs

	def picard_merge_sam_files(self):
		"""
		BAM readset files are merged into one file per sample. Merge is done using [Picard](http://broadinstitute.github.io/picard/).
		"""

		jobs = []
		for sample in self.samples:
			# Skip samples with one readset only, since symlink has been created at align step
			if len(sample.readsets) > 1:
				alignment_directory = os.path.join("alignment", sample.name)
				inputs = [os.path.join(alignment_directory, readset.name, "Aligned.sortedByCoord.out.bam") for readset in sample.readsets]
				output = os.path.join(alignment_directory, sample.name + ".sorted.bam")

				job = picard.merge_sam_files(inputs, output)
				job.name = "picard_merge_sam_files." + sample.name
				jobs.append(job)
		return jobs

	def picard_sort_sam(self):
		"""
		The alignment file is reordered (QueryName) using [Picard](http://broadinstitute.github.io/picard/). The QueryName-sorted bam files will be used to determine raw read counts.
		"""

		jobs = []
		for sample in self.samples:
			alignment_file_prefix = os.path.join("alignment", sample.name, sample.name)

			job = picard.sort_sam(
				alignment_file_prefix + ".sorted.bam",
				alignment_file_prefix + ".QueryNameSorted.bam",
				"queryname"
			)
			job.name = "picard_sort_sam." + sample.name
			jobs.append(job)
		return jobs

	def picard_mark_duplicates(self):
		"""
		Mark duplicates. Aligned reads per sample are duplicates if they have the same 5' alignment positions
		(for both mates in the case of paired-end reads). All but the best pair (based on alignment score)
		will be marked as a duplicate in the BAM file. Marking duplicates is done using [Picard](http://broadinstitute.github.io/picard/).
		"""

		jobs = []
		for sample in self.samples:
			alignment_file_prefix = os.path.join("alignment", sample.name, sample.name + ".sorted.")

			job = picard.mark_duplicates(
				[alignment_file_prefix + "bam"],
				alignment_file_prefix + "mdup.bam",
				alignment_file_prefix + "mdup.metrics"
			)
			job.name = "picard_mark_duplicates." + sample.name
			jobs.append(job)
		return jobs

	def bam_hard_clip(self):
		"""
		Generate a hardclipped version of the bam for the toxedo suite which doesn't support this official sam feature.
		"""
		
		jobs = []
		for sample in self.samples:
			alignment_input = os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.bam")
			alignment_output = os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.hardClip.bam")
			job=pipe_jobs([
				samtools.view(
					alignment_input,
					None,
					"-h"
				),
				Job(
					[None],
					[alignment_output],
					# awk to transform soft clip into hard clip for tuxedo suite
					command="""\
awk 'BEGIN {{OFS="\\t"}} {{if (substr($1,1,1)=="@") {{print;next}}; split($6,C,/[0-9]*/); split($6,L,/[SMDIN]/); if (C[2]=="S") {{$10=substr($10,L[1]+1); $11=substr($11,L[1]+1)}}; if (C[length(C)]=="S") {{L1=length($10)-L[length(L)-1]; $10=substr($10,1,L1); $11=substr($11,1,L1); }}; gsub(/[0-9]*S/,"",$6); print}}' """.format()
				),
				samtools.view(
					"-",
					alignment_output,
					"-hbS"
				),
			])
			job.name="tuxedo_hard_clip."+ sample.name
			jobs.append(job)
		return jobs
	

	def rnaseqc(self):
		"""
		Computes a series of quality control metrics using [RNA-SeQC](https://www.broadinstitute.org/cancer/cga/rna-seqc).
		"""

		jobs = []
		sample_file = os.path.join("alignment", "rnaseqc.samples.txt")
		sample_rows = [[sample.name, os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.bam"), "RNAseq"] for sample in self.samples]
		input_bams = [sample_row[1] for sample_row in sample_rows]
		output_directory = os.path.join("metrics", "rnaseqRep")
		# Use GTF with transcript_id only otherwise RNASeQC fails
		gtf_transcript_id = config.param('rnaseqc', 'gtf_transcript_id', type='filepath')

		jobs.append(concat_jobs([
			Job(command="mkdir -p " + output_directory, removable_files=[output_directory]),
			Job(input_bams, [sample_file], command="""\
echo "Sample\tBamFile\tNote
{sample_rows}" \\
  > {sample_file}""".format(sample_rows="\n".join(["\t".join(sample_row) for sample_row in sample_rows]), sample_file=sample_file)),
			metrics.rnaseqc(sample_file, output_directory, self.run_type == "SINGLE_END", gtf_file=gtf_transcript_id),
			Job([], [output_directory + ".zip"], command="zip -r {output_directory}.zip {output_directory}".format(output_directory=output_directory))
		], name="rnaseqc"))

		trim_metrics_file = os.path.join("metrics", "trimSampleTable.tsv")
		metrics_file = os.path.join("metrics", "rnaseqRep", "metrics.tsv")
		report_metrics_file = os.path.join("report", "trimAlignmentTable.tsv")
		report_file = os.path.join("report", "RnaSeq.rnaseqc.md")
		jobs.append(
			Job(
				[metrics_file],
				[report_file],
				[['rnaseqc', 'module_python'], ['rnaseqc', 'module_pandoc']],
				# Ugly awk to merge sample metrics with trim metrics if they exist; knitr may do this better
				command="""\
mkdir -p report && \\
cp {output_directory}.zip report/reportRNAseqQC.zip && \\
python -c 'import csv; csv_in = csv.DictReader(open("{metrics_file}"), delimiter="\t")
print "\t".join(["Sample", "Aligned Reads", "Alternative Alignments", "%", "rRNA Reads", "Coverage", "Exonic Rate", "Genes"])
print "\\n".join(["\t".join([
	line["Sample"],
	line["Mapped"],
	line["Alternative Aligments"],
	str(float(line["Alternative Aligments"]) / float(line["Mapped"]) * 100),
	line["rRNA"],
	line["Mean Per Base Cov."],
	line["Exonic Rate"],
	line["Genes Detected"]
]) for line in csv_in])' \\
  > {report_metrics_file}.tmp && \\
if [[ -f {trim_metrics_file} ]]
then
  awk -F"\t" 'FNR==NR{{raw_reads[$1]=$2; surviving_reads[$1]=$3; surviving_pct[$1]=$4; next}}{{OFS="\t"; if ($2=="Aligned Reads"){{surviving_pct[$1]="%"; aligned_pct="%"; rrna_pct="%"}} else {{aligned_pct=($2 / surviving_reads[$1] * 100); rrna_pct=($5 / surviving_reads[$1] * 100)}}; printf $1"\t"raw_reads[$1]"\t"surviving_reads[$1]"\t"surviving_pct[$1]"\t"$2"\t"aligned_pct"\t"$3"\t"$4"\t"$5"\t"rrna_pct; for (i = 6; i<= NF; i++) {{printf "\t"$i}}; print ""}}' \\
  {trim_metrics_file} \\
  {report_metrics_file}.tmp \\
  > {report_metrics_file}
else
  cp {report_metrics_file}.tmp {report_metrics_file}
fi && \\
rm {report_metrics_file}.tmp && \\
trim_alignment_table_md=`if [[ -f {trim_metrics_file} ]] ; then cut -f1-13 {report_metrics_file} | LC_NUMERIC=en_CA awk -F "\t" '{{OFS="|"; if (NR == 1) {{$1 = $1; print $0; print "-----|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:"}} else {{print $1, sprintf("%\\47d", $2), sprintf("%\\47d", $3), sprintf("%.1f", $4), sprintf("%\\47d", $5), sprintf("%.1f", $6), sprintf("%\\47d", $7), sprintf("%.1f", $8), sprintf("%\\47d", $9), sprintf("%.1f", $10), sprintf("%.2f", $11), sprintf("%.2f", $12), sprintf("%\\47d", $13)}}}}' ; else cat {report_metrics_file} | LC_NUMERIC=en_CA awk -F "\t" '{{OFS="|"; if (NR == 1) {{$1 = $1; print $0; print "-----|-----:|-----:|-----:|-----:|-----:|-----:|-----:"}} else {{print $1, sprintf("%\\47d", $2), sprintf("%\\47d", $3), sprintf("%.1f", $4), sprintf("%\\47d", $5), sprintf("%.2f", $6), sprintf("%.2f", $7), $8}}}}' ; fi`
pandoc \\
  {report_template_dir}/{basename_report_file} \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable trim_alignment_table="$trim_alignment_table_md" \\
  --to markdown \\
  > {report_file}""".format(
					output_directory=output_directory,
					report_template_dir=self.report_template_dir,
					trim_metrics_file=trim_metrics_file,
					metrics_file=metrics_file,
					basename_report_file=os.path.basename(report_file),
					report_metrics_file=report_metrics_file,
					report_file=report_file
				),
				report_files=[report_file],
				name="rnaseqc_report"
			)
		)

		return jobs


	def picard_rna_metrics(self):
		"""
		Computes a series of quality control metrics using both CollectRnaSeqMetrics and CollectAlignmentSummaryMetrics functions
		metrics are collected using [Picard](http://broadinstitute.github.io/picard/).
		"""
		
		jobs = []
		reference_file = config.param('picard_rna_metrics', 'genome_fasta', type='filepath')
		for sample in self.samples:
				alignment_file = os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.bam")
				output_directory = os.path.join("metrics", sample.name)
				
				job = concat_jobs([
						Job(command="mkdir -p " + output_directory, removable_files=[output_directory]),
						picard.collect_multiple_metrics(alignment_file, os.path.join(output_directory,sample.name),reference_file),
						picard.collect_rna_metrics(alignment_file, os.path.join(output_directory,sample.name+".picard_rna_metrics"))
				],name="picard_rna_metrics."+ sample.name)
				jobs.append(job)
		
		return jobs

	def estimate_ribosomal_rna(self):
		"""
		Use bwa mem to align reads on the rRNA reference fasta and count the number of read mapped
		The filtered reads are aligned to a reference fasta file of ribosomal sequence. The alignment is done per sequencing readset.
		The alignment software used is [BWA](http://bio-bwa.sourceforge.net/) with algorithm: bwa mem.
		BWA output BAM files are then sorted by coordinate using [Picard](http://broadinstitute.github.io/picard/).

		This step takes as input files:

		readset Bam files
		"""

		jobs = []
		for readset in self.readsets:
			readset_bam = os.path.join("alignment", readset.sample.name, readset.name , "Aligned.sortedByCoord.out.bam")
			output_folder = os.path.join("metrics",readset.sample.name, readset.name)
			readset_metrics_bam = os.path.join(output_folder,readset.name +"rRNA.bam")


			job = concat_jobs([
				Job(command="mkdir -p " + os.path.dirname(readset_bam) + " " + output_folder),
				pipe_jobs([
					bvatools.bam2fq(
						readset_bam
					),
					bwa.mem(
						"/dev/stdin",
						None,
						read_group="'@RG" + \
							"\tID:" + readset.name + \
							"\tSM:" + readset.sample.name + \
							("\tLB:" + readset.library if readset.library else "") + \
							("\tPU:run" + readset.run + "_" + readset.lane if readset.run and readset.lane else "") + \
							("\tCN:" + config.param('bwa_mem_rRNA', 'sequencing_center') if config.param('bwa_mem_rRNA', 'sequencing_center', required=False) else "") + \
							"\tPL:Illumina" + \
							"'",
						ref=config.param('bwa_mem_rRNA', 'ribosomal_fasta'),
						ini_section='bwa_mem_rRNA'
					),
					picard.sort_sam(
						"/dev/stdin",
						readset_metrics_bam,
						"coordinate",
						ini_section='picard_sort_sam_rrna'
					)
				]),
				tools.py_rrnaBAMcount (
					bam=readset_metrics_bam, 
					gtf=config.param('bwa_mem_rRNA', 'gtf'), 
					output=os.path.join(output_folder,readset.name+"rRNA.stats.tsv"),
					typ="transcript")], name="bwa_mem_rRNA." + readset.name )
			
			job.removable_files=[readset_metrics_bam]
			jobs.append(job)
		return jobs

		
	def wiggle(self):
		"""
		Generate wiggle tracks suitable for multiple browsers.
		"""

		jobs = []
		
		##check the library status
		library = {}
		for readset in self.readsets:
			if not library.has_key(readset.sample) :
				library[readset.sample]="PAIRED_END"
			if readset.run_type == "SINGLE_END" :
				library[readset.sample]="SINGLE_END"
		
		for sample in self.samples:
			bam_file_prefix = os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.")
			input_bam = bam_file_prefix + "bam"
			bed_graph_prefix = os.path.join("tracks", sample.name, sample.name)
			big_wig_prefix = os.path.join("tracks", "bigWig", sample.name)

			if (config.param('DEFAULT', 'strand_info') != 'fr-unstranded') and library[sample] == "PAIRED_END":
				input_bam_f1 = bam_file_prefix + "tmp1.forward.bam"
				input_bam_f2 = bam_file_prefix + "tmp2.forward.bam"
				input_bam_r1 = bam_file_prefix + "tmp1.reverse.bam"
				input_bam_r2 = bam_file_prefix + "tmp2.reverse.bam"
				output_bam_f = bam_file_prefix + "forward.bam"
				output_bam_r = bam_file_prefix + "reverse.bam"

				bam_f_job = concat_jobs([
					samtools.view(input_bam, input_bam_f1, "-bh -F 256 -f 81"),
					samtools.view(input_bam, input_bam_f2, "-bh -F 256 -f 161"),
					picard.merge_sam_files([input_bam_f1, input_bam_f2], output_bam_f),
					Job(command="rm " + input_bam_f1 + " " + input_bam_f2)
				], name="wiggle." + sample.name + ".forward_strandspec")
				# Remove temporary-then-deleted files from job output files, otherwise job is never up to date
				bam_f_job.output_files.remove(input_bam_f1)
				bam_f_job.output_files.remove(input_bam_f2)

				bam_r_job = concat_jobs([
					Job(command="mkdir -p " + os.path.join("tracks", sample.name) + " " + os.path.join("tracks", "bigWig")),
					samtools.view(input_bam, input_bam_r1, "-bh -F 256 -f 97"),
					samtools.view(input_bam, input_bam_r2, "-bh -F 256 -f 145"),
					picard.merge_sam_files([input_bam_r1, input_bam_r2], output_bam_r),
					Job(command="rm " + input_bam_r1 + " " + input_bam_r2)
				], name="wiggle." + sample.name + ".reverse_strandspec")
				# Remove temporary-then-deleted files from job output files, otherwise job is never up to date
				bam_r_job.output_files.remove(input_bam_r1)
				bam_r_job.output_files.remove(input_bam_r2)

				jobs.extend([bam_f_job, bam_r_job])

				outputs = [
					[bed_graph_prefix + ".forward.bedGraph", big_wig_prefix + ".forward.bw"],
					[bed_graph_prefix + ".reverse.bedGraph", big_wig_prefix + ".reverse.bw"],
				]
			else:
				outputs = [[bed_graph_prefix + ".bedGraph", big_wig_prefix + ".bw"]]

			for bed_graph_output, big_wig_output in outputs:
				job = concat_jobs([
					Job(command="mkdir -p " + os.path.join("tracks", sample.name) + " " + os.path.join("tracks", "bigWig"), removable_files=["tracks"]),
					bedtools.graph(input_bam, bed_graph_output, big_wig_output,library[sample])
				], name="wiggle." + re.sub(".bedGraph", "", os.path.basename(bed_graph_output)))
				jobs.append(job)

		return jobs

	def raw_counts(self):
		"""
		Count reads in features using [htseq-count](http://www-huber.embl.de/users/anders/HTSeq/doc/count.html).
		"""

		jobs = []

		for sample in self.samples:
			alignment_file_prefix = os.path.join("alignment", sample.name, sample.name)
			input_bam = alignment_file_prefix + ".QueryNameSorted.bam"
			
			# Count reads
			output_count = os.path.join("raw_counts", sample.name + ".readcounts.csv")
			stranded = "no" if config.param('DEFAULT', 'strand_info') == "fr-unstranded" else "reverse"
			job = concat_jobs([
				Job(command="mkdir -p raw_counts"),
				pipe_jobs([
						samtools.view(
								input_bam,
								options="-F 4"
						),
						htseq.htseq_count(
						"-",
						config.param('htseq_count', 'gtf', type='filepath'),
						output_count,
						config.param('htseq_count', 'options'),
						stranded
						)
				])
			], name="htseq_count." + sample.name)
			jobs.append(job)

		return jobs

	def raw_counts_metrics(self):
		"""
		Create rawcount matrix, zip the wiggle tracks and create the saturation plots based on standardized read counts.
		"""

		jobs = []

		# Create raw count matrix
		output_directory = "DGE"
		read_count_files = [os.path.join("raw_counts", sample.name + ".readcounts.csv") for sample in self.samples]
		output_matrix = os.path.join(output_directory, "rawCountMatrix.csv")

		job = Job(read_count_files, [output_matrix], [['raw_counts_metrics', 'module_mugqic_tools']], name="metrics.matrix")

		job.command = """\
mkdir -p {output_directory} && \\
gtf2tmpMatrix.awk \\
  {reference_gtf} \\
  {output_directory}/tmpMatrix.txt && \\
HEAD='Gene\\tSymbol' && \\
for read_count_file in \\
  {read_count_files}
do
  sort -k1,1 $read_count_file > {output_directory}/tmpSort.txt && \\
  join -1 1 -2 1 <(sort -k1,1 {output_directory}/tmpMatrix.txt) {output_directory}/tmpSort.txt > {output_directory}/tmpMatrix.2.txt && \\
  mv {output_directory}/tmpMatrix.2.txt {output_directory}/tmpMatrix.txt && \\
  na=$(basename $read_count_file | cut -d. -f1) && \\
  HEAD="$HEAD\\t$na"
done && \\
echo -e $HEAD | cat - {output_directory}/tmpMatrix.txt | tr ' ' '\\t' > {output_matrix} && \\
rm {output_directory}/tmpSort.txt {output_directory}/tmpMatrix.txt""".format(
			reference_gtf=config.param('raw_counts_metrics', 'gtf', type='filepath'),
			output_directory=output_directory,
			read_count_files=" \\\n  ".join(read_count_files),
			output_matrix=output_matrix
		)
		jobs.append(job)

		# Create Wiggle tracks archive
		library = {}
		for readset in self.readsets:
			if not library.has_key(readset.sample) :
				library[readset.sample]="PAIRED_END"
			if readset.run_type == "SINGLE_END" :
				library[readset.sample]="SINGLE_END"
				
		wiggle_directory = os.path.join("tracks", "bigWig")
		wiggle_archive = "tracks.zip"
		big_wig_prefix = os.path.join("tracks", "bigWig", sample.name)
		if config.param('DEFAULT', 'strand_info') != 'fr-unstranded' and library[sample] == "PAIRED_END" :
			wiggle_files = []
			for sample in self.samples:
				wiggle_files.extend([os.path.join(wiggle_directory, sample.name) + ".forward.bw", os.path.join(wiggle_directory, sample.name) + ".reverse.bw"])
		else:
			wiggle_files = [os.path.join(wiggle_directory, sample.name + ".bw") for sample in self.samples]
		jobs.append(Job(wiggle_files, [wiggle_archive], name="metrics.wigzip", command="zip -r " + wiggle_archive + " " + wiggle_directory))

		# RPKM and Saturation
		count_file = os.path.join("DGE", "rawCountMatrix.csv")
		gene_size_file = config.param('rpkm_saturation', 'gene_size', type='filepath')
		rpkm_directory = "raw_counts"
		saturation_directory = os.path.join("metrics", "saturation")

		job = concat_jobs([
			Job(command="mkdir -p " + saturation_directory),
			metrics.rpkm_saturation(count_file, gene_size_file, rpkm_directory, saturation_directory)
		], name="rpkm_saturation")
		jobs.append(job)

		report_file = os.path.join("report", "RnaSeq.raw_counts_metrics.md")
		jobs.append(
			Job(
				[wiggle_archive, saturation_directory + ".zip","metrics/rnaseqRep/corrMatrixSpearman.txt"],
				[report_file],
				[['raw_counts_metrics', 'module_pandoc']],
				command="""\
mkdir -p report && \\
cp metrics/rnaseqRep/corrMatrixSpearman.txt report/corrMatrixSpearman.tsv && \\
cp {wiggle_archive} report/ && \\
cp {saturation_archive} report/ && \\
pandoc --to=markdown \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable corr_matrix_spearman_table="`head -16 report/corrMatrixSpearman.tsv | cut -f-16| awk -F"\t" '{{OFS="\t"; if (NR==1) {{$0="Vs"$0; print; gsub(/[^\t]/, "-"); print}} else {{printf $1; for (i=2; i<=NF; i++) {{printf "\t"sprintf("%.2f", $i)}}; print ""}}}}' | sed 's/\t/|/g'`" \\
  {report_template_dir}/{basename_report_file} \\
  > {report_file}""".format(
					wiggle_archive=wiggle_archive,
					saturation_archive=saturation_directory + ".zip",
					report_template_dir=self.report_template_dir,
					basename_report_file=os.path.basename(report_file),
					report_file=report_file
				),
				report_files=[report_file],
				name="raw_count_metrics_report")
		)

		return jobs

	def cufflinks(self):
		"""
		Compute RNA-Seq data expression using [cufflinks](http://cole-trapnell-lab.github.io/cufflinks/cufflinks/).
		Warning: It needs to use a hard clipped bam file while Tuxedo tools do not support official soft clip SAM format
		"""

		jobs = []
		
		gtf = config.param('cufflinks','gtf', type='filepath')

		for sample in self.samples:
			input_bam = os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.hardClip.bam")
			output_directory = os.path.join("cufflinks", sample.name)

			# De Novo FPKM
			job = cufflinks.cufflinks(input_bam, output_directory, gtf)
			job.removable_files = ["cufflinks"]
			job.name = "cufflinks."+sample.name
			jobs.append(job)

		return jobs
	
	def cuffmerge(self):
		"""
		Merge assemblies into a master transcriptome reference using [cuffmerge](http://cole-trapnell-lab.github.io/cufflinks/cuffmerge/).
		"""

		output_directory = os.path.join("cufflinks", "AllSamples")
		sample_file = os.path.join("cufflinks", "cuffmerge.samples.txt")
		input_gtfs = [os.path.join("cufflinks", sample.name, "transcripts.gtf") for sample in self.samples]
		gtf = config.param('cuffmerge','gtf', type='filepath')
		
		
		job = concat_jobs([
			Job(command="mkdir -p " + output_directory),
			Job(input_gtfs, [sample_file], command="""\
`cat > {sample_file} << END
{sample_rows}
END
  
`""".format(sample_rows="\n".join(input_gtfs), sample_file=sample_file)),
			cufflinks.cuffmerge(sample_file, output_directory, gtf_file=gtf)],
			name="cuffmerge")
		
		return [job]
		
	def cuffquant(self):
		"""
		Compute expression profiles (abundances.cxb) using [cuffquant](http://cole-trapnell-lab.github.io/cufflinks/cuffquant/).
		Warning: It needs to use a hard clipped bam file while Tuxedo tools do not support official soft clip SAM format
		"""

		jobs = []
		
		gtf = os.path.join("cufflinks", "AllSamples","merged.gtf")
		
		for sample in self.samples:
			input_bam = os.path.join("alignment", sample.name, sample.name + ".sorted.mdup.hardClip.bam")
			output_directory = os.path.join("cufflinks", sample.name)

			#Quantification
			job = cufflinks.cuffquant(input_bam, output_directory, gtf)
			job.name = "cuffquant."+sample.name
			jobs.append(job)

		return jobs
	
	def cuffdiff(self):
		"""
		[Cuffdiff](http://cole-trapnell-lab.github.io/cufflinks/cuffdiff/) is used to calculate differential transcript expression levels and test them for significant differences.
		"""

		jobs = []

		fpkm_directory = "cufflinks"
		gtf = os.path.join(fpkm_directory, "AllSamples","merged.gtf")


		# Perform cuffdiff on each design contrast
		for contrast in self.contrasts:
			job = cufflinks.cuffdiff(
				# Cuffdiff input is a list of lists of replicate bams per control and per treatment
				[[os.path.join(fpkm_directory, sample.name, "abundances.cxb") for sample in group] for group in contrast.controls, contrast.treatments],
				gtf,
				os.path.join("cuffdiff", contrast.name)
			)
			job.removable_files = ["cuffdiff"]
			job.name = "cuffdiff." + contrast.name
			jobs.append(job)

		return jobs
	
	def cuffnorm(self):
		"""
		Global normalization of RNA-Seq expression levels using [Cuffnorm](http://cole-trapnell-lab.github.io/cufflinks/cuffnorm/).
		"""

		jobs = []

		fpkm_directory = "cufflinks"
		gtf = os.path.join(fpkm_directory, "AllSamples","merged.gtf")
		sample_labels = ",".join([sample.name for sample in self.samples])

		# Perform cuffnorm using every samples
		job = cufflinks.cuffnorm([os.path.join(fpkm_directory, sample.name, "abundances.cxb") for sample in self.samples],
			 gtf,
			 "cuffnorm",sample_labels)
		job.removable_files = ["cuffnorm"]
		job.name = "cuffnorm" 
		jobs.append(job)

		return jobs

	def fpkm_correlation_matrix(self):
		"""
		Compute the pearson corrleation matrix of gene and transcripts FPKM. FPKM data are those estimated by cuffnorm.
		"""
		output_directory = "metrics"
		output_transcript = os.path.join(output_directory,"transcripts_fpkm_correlation_matrix.tsv")
		cuffnorm_transcript = os.path.join("cuffnorm","isoforms.fpkm_table")
		output_gene = os.path.join(output_directory,"gene_fpkm_correlation_matrix.tsv")
		cuffnorm_gene = os.path.join("cuffnorm","genes.fpkm_table")
		
		jobs = []
	  
		job = concat_jobs([Job(command="mkdir -p " + output_directory),
						   utils.utils.fpkm_correlation_matrix(cuffnorm_transcript, output_transcript)])
		job.name="fpkm_correlation_matrix_transcript"
		jobs = jobs + [job]
		job = utils.utils.fpkm_correlation_matrix(cuffnorm_gene, output_gene)
		job.name="fpkm_correlation_matrix_gene"
		jobs = jobs + [job]
		
		return jobs

	def gq_seq_utils_exploratory_analysis_rnaseq(self):
		"""
		Exploratory analysis using the gqSeqUtils R package.
		"""

		jobs = []

		# gqSeqUtils function call
		sample_fpkm_readcounts = [[
			sample.name,
			os.path.join("cufflinks", sample.name, "isoforms.fpkm_tracking"),
			os.path.join("raw_counts", sample.name + ".readcounts.csv")
		] for sample in self.samples]
		jobs.append(concat_jobs([
			Job(command="mkdir -p exploratory"),
			gq_seq_utils.exploratory_analysis_rnaseq(
				os.path.join("DGE", "rawCountMatrix.csv"),
				"cuffnorm",
				config.param('gq_seq_utils_exploratory_analysis_rnaseq', 'genes', type='filepath'),
				"exploratory"
			)
		], name="gq_seq_utils_exploratory_analysis_rnaseq"))

		# Render Rmarkdown Report
		jobs.append(
			rmarkdown.render(
			 job_input			= os.path.join("exploratory", "index.tsv"),
			 job_name			 = "gq_seq_utils_exploratory_analysis_rnaseq_report",
			 input_rmarkdown_file = os.path.join(self.report_template_dir, "RnaSeq.gq_seq_utils_exploratory_analysis_rnaseq.Rmd") ,
			 render_output_dir	= 'report',
			 module_section	   = 'report', # TODO: this or exploratory?
			 prerun_r			 = 'report_dir="report";' # TODO: really necessary or should be hard-coded in exploratory.Rmd?
			 )
		)



		report_file = os.path.join("report", "RnaSeq.cuffnorm.md")
		jobs.append(
			Job(
				[os.path.join("cufflinks", "AllSamples","merged.gtf")],
				[report_file],
				command="""\
mkdir -p report && \\
zip -r report/cuffAnalysis.zip cufflinks/ cuffdiff/ cuffnorm/ && \\
cp \\
  {report_template_dir}/{basename_report_file} \\
  {report_file}""".format(
					report_template_dir=self.report_template_dir,
					basename_report_file=os.path.basename(report_file),
					report_file=report_file
				),
				report_files=[report_file],
				name="cuffnorm_report")
		)

		return jobs

	def differential_expression(self):
		"""
		Performs differential gene expression analysis using [DESEQ](http://bioconductor.org/packages/release/bioc/html/DESeq.html) and [EDGER](http://www.bioconductor.org/packages/release/bioc/html/edgeR.html).
		Merge the results of the analysis in a single csv file.
		"""

		# If --design <design_file> option is missing, self.contrasts call will raise an Exception
		if self.contrasts:
			design_file = os.path.relpath(self.args.design.name, self.output_dir)
		output_directory = "DGE"
		count_matrix = os.path.join(output_directory, "rawCountMatrix.csv")

		edger_job = differential_expression.edger(design_file, count_matrix, output_directory)
		edger_job.output_files = [os.path.join(output_directory, contrast.name, "edger_results.csv") for contrast in self.contrasts]

		deseq_job = differential_expression.deseq(design_file, count_matrix, output_directory)
		deseq_job.output_files = [os.path.join(output_directory, contrast.name, "dge_results.csv") for contrast in self.contrasts]

		return [concat_jobs([
			Job(command="mkdir -p " + output_directory),
			edger_job,
			deseq_job
		], name="differential_expression")]

	def differential_expression_goseq(self):
		"""
		Gene Ontology analysis for RNA-Seq using the Bioconductor's R package [goseq](http://www.bioconductor.org/packages/release/bioc/html/goseq.html).
		Generates GO annotations for differential gene expression analysis.
		"""

		jobs = []

		for contrast in self.contrasts:
			# goseq for differential gene expression results
			job = differential_expression.goseq(
				os.path.join("DGE", contrast.name, "dge_results.csv"),
				config.param("differential_expression_goseq", "dge_input_columns"),
				os.path.join("DGE", contrast.name, "gene_ontology_results.csv")
			)
			job.name = "differential_expression_goseq.dge." + contrast.name
			jobs.append(job)


###################
		report_file = os.path.join("report", "RnaSeq.differential_expression.md")
		jobs.append(
			Job(
				[os.path.join("DGE", "rawCountMatrix.csv")] +
				[os.path.join("DGE", contrast.name, "dge_results.csv") for contrast in self.contrasts] +
				[os.path.join("cuffdiff", contrast.name, "isoforms.fpkm_tracking") for contrast in self.contrasts] +
				[os.path.join("cuffdiff", contrast.name, "isoform_exp.diff") for contrast in self.contrasts] +
				[os.path.join("DGE", contrast.name, "gene_ontology_results.csv") for contrast in self.contrasts],
				[report_file],
				[['rnaseqc', 'module_pandoc']],
				# Ugly awk to format differential expression results into markdown for genes, transcripts and GO if any; knitr may do this better
				# Ugly awk and python to merge cuffdiff fpkm and isoforms into transcript expression results
				command="""\
mkdir -p report && \\
cp {design_file} report/design.tsv && \\
cp DGE/rawCountMatrix.csv report/ && \\
pandoc \\
  {report_template_dir}/{basename_report_file} \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable design_table="`head -7 report/design.tsv | cut -f-8 | awk -F"\t" '{{OFS="\t"; if (NR==1) {{print; gsub(/[^\t]/, "-")}} print}}' | sed 's/\t/|/g'`" \\
  --variable raw_count_matrix_table="`head -7 report/rawCountMatrix.csv | cut -f-8 | awk -F"\t" '{{OFS="\t"; if (NR==1) {{print; gsub(/[^\t]/, "-")}} print}}' | sed 's/\t/|/g'`" \\
  --to markdown \\
  > {report_file} && \\
for contrast in {contrasts}
do
  mkdir -p report/DiffExp/$contrast/
  echo -e "\\n#### $contrast Results\\n" >> {report_file}
  cp DGE/$contrast/dge_results.csv report/DiffExp/$contrast/${{contrast}}_Genes_DE_results.tsv
  echo -e "\\nTable: Differential Gene Expression Results (**partial table**; [download full table](DiffExp/$contrast/${{contrast}}_Genes_DE_results.tsv))\\n" >> {report_file}
  head -7 report/DiffExp/$contrast/${{contrast}}_Genes_DE_results.tsv | cut -f-8 | sed '2i ---\t---\t---\t---\t---\t---\t---\t---' | sed 's/\t/|/g' >> {report_file}
  sed '1s/^tracking_id/test_id/' cuffdiff/$contrast/isoforms.fpkm_tracking | awk -F"\t" 'FNR==NR{{line[$1]=$0; next}}{{OFS="\t"; print line[$1], $0}}' - cuffdiff/$contrast/isoform_exp.diff | python -c 'import csv,sys; rows_in = csv.DictReader(sys.stdin, delimiter="\t"); rows_out = csv.DictWriter(sys.stdout, fieldnames=["test_id", "gene_id", "tss_id","nearest_ref_id","class_code","gene","locus","length","log2(fold_change)","test_stat","p_value","q_value"], delimiter="\t", extrasaction="ignore"); rows_out.writeheader(); rows_out.writerows(rows_in)' > report/DiffExp/$contrast/${{contrast}}_Transcripts_DE_results.tsv
  echo -e "\\n---\\n\\nTable: Differential Transcript Expression Results (**partial table**; [download full table](DiffExp/$contrast/${{contrast}}_Transcripts_DE_results.tsv))\\n" >> {report_file}
  head -7 report/DiffExp/$contrast/${{contrast}}_Transcripts_DE_results.tsv | cut -f-8 | sed '2i ---\t---\t---\t---\t---\t---\t---\t---' | sed 's/\t/|/g' >> {report_file}
  if [ `wc -l DGE/$contrast/gene_ontology_results.csv | cut -f1 -d\ ` -gt 1 ]
  then
	cp DGE/$contrast/gene_ontology_results.csv report/DiffExp/$contrast/${{contrast}}_Genes_GO_results.tsv
	echo -e "\\n---\\n\\nTable: GO Results of the Differentially Expressed Genes (**partial table**; [download full table](DiffExp/${{contrast}}/${{contrast}}_Genes_GO_results.tsv))\\n" >> {report_file}
	head -7 report/DiffExp/${{contrast}}/${{contrast}}_Genes_GO_results.tsv | cut -f-8 | sed '2i ---\t---\t---\t---\t---\t---\t---\t---' | sed 's/\t/|/g' >> {report_file}
  else
	echo -e "\\nNo FDR adjusted GO enrichment was significant (p-value too high) based on the differentially expressed gene results for this design.\\n" >> {report_file}
  fi
done""".format(
					design_file=os.path.abspath(self.args.design.name),
					report_template_dir=self.report_template_dir,
					basename_report_file=os.path.basename(report_file),
					report_file=report_file,
					contrasts=" ".join([contrast.name for contrast in self.contrasts])
				),
				report_files=[report_file],
				name="differential_expression_goseq_report")
		)

############

		return jobs

########### Fusion functions ###########
	def select_input_fastq(self, sample):
		"""
		According to readset select input fastqs for fusion callers.
		"""
		input_dir = os.path.join("fusions", "gunzip_fastq", sample.name)
		if len(sample.readsets) > 1: # sample has more than 1 readset, use merged fastq
			fastq1 = os.path.join(input_dir, "merged.pair1.fastq")
			fastq2 = os.path.join(input_dir, "merged.pair2.fastq")
		else:
			#input filse are bams
			readset = sample.readsets[0]
			if readset.bam:
				fastq1 = os.path.join(input_dir, os.path.basename(re.sub("\.bam$", ".pair1.fastq", readset.bam)))
				fastq2 = os.path.join(input_dir, os.path.basename(re.sub("\.bam$", ".pair2.fastq", readset.bam)))

				fastq1 = os.path.join("fusions","star_filter", sample.name, os.path.basename(readset.bam)+".star.filtered.pair1.fastq") 
				fastq2 = os.path.join("fusions","star_filter", sample.name, os.path.basename(readset.bam)+".star.filtered.pair2.fastq") 
			if readset.fastq1:
				if readset.fastq1.endswith(".gz"):
					# input fles are gizped fastqs
					fastq1 = os.path.join(input_dir, os.path.basename(re.sub("\.gz$", "", readset.fastq1)))
					fastq2 = os.path.join(input_dir, os.path.basename(re.sub("\.gz$", "", readset.fastq2)))
				else:
					# input fles are fastqs
					fastq1 = os.path.join(input_dir, os.path.basename(readset.fastq1))
					fastq2 = os.path.join(input_dir, os.path.basename(readset.fastq2))
		return fastq1, fastq2	

	def defuse(self):
		"""
		Run Defuse to call gene fusion
		"""
		jobs = []
		for sample in self.samples:
			fastq1, fastq2 = self.select_input_fastq(sample)
			out_dir = os.path.join("fusions", "defuse", sample.name)
			defuse_job = defuse.defuse(fastq1, fastq2, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				defuse_job
			], name="defuse." + sample.name)

			jobs.append(job)

		return jobs
	def star_filter(self):
		"""
		filter out STAR uniquely aligned reads and convert the rest to fastq
		currently works with STAR query name sorted bam generated by original RNA-SEQ pipeline
		"""
		jobs = []
		for readset in self.readsets:
			out_dir = os.path.join("fusions", "star_filter", readset.sample.name)
			if readset.bam:
				star_filter_job = star_filter.star_filter(readset.bam, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				star_filter_job
			], name="star_filter." + readset.sample.name)

			jobs.append(job)

		return jobs

	def gunzip_fastq(self):
		"""
		Gunzip .fastq.gz files 
		"""
		jobs = []
		for readset in self.readsets:
			trim_file_prefix = os.path.join("trim", readset.sample.name, readset.name + ".trim.")
			out_dir = os.path.join("fusions", "gunzip_fastq", readset.sample.name)
			# Find input readset FASTQs first from previous trimmomatic job, then from original FASTQs in the readset sheet
			if readset.run_type == "PAIRED_END":
				candidate_input_files = [[trim_file_prefix + "pair1.fastq.gz", trim_file_prefix + "pair2.fastq.gz"]]
				if readset.fastq1 and readset.fastq2:
					candidate_input_files.append([readset.fastq1, readset.fastq2])
				if readset.bam:
					candidate_input_files.append([re.sub("\.bam$", ".pair1.fastq.gz", readset.bam), re.sub("\.bam$", ".pair2.fastq.gz", readset.bam)])
				[fastq1, fastq2] = self.select_input_files(candidate_input_files)
			else:
				raise Exception("Error: run type \"" + readset.run_type +
				"\" is invalid for readset \"" + readset.name + "\" (should be PAIRED_END)!")
			gunzip1_job = gunzip.gunzip_fastq(fastq1, out_dir)
			gunzip2_job = gunzip.gunzip_fastq(fastq2, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				gunzip1_job,
				gunzip2_job
			], name="gunzip_fastq." + readset.name)

			jobs.append(job)

		return jobs
	def merge_fastq(self):
		"""
		Merge fastqs of the same sample
		"""
		jobs = []
		for sample in self.samples:
			if len(sample.readsets) > 1:
				input_dir = os.path.join("fusions", "gunzip_fastq", sample.name)
				fastq1_list = []
				fastq2_list = []
				for readset in sample.readsets:
					if readset.bam:
						fastq1 = os.path.join(input_dir, os.path.basename(re.sub("\.bam$", ".pair1.fastq", readset.bam)))
						fastq2 = os.path.join(input_dir, os.path.basename(re.sub("\.bam$", ".pair2.fastq", readset.bam)))
					if readset.fastq1:
						if readset.fastq1.endswith(".gz"):
							# input fles are gizped fastqs
							fastq1 = os.path.join(input_dir, os.path.basename(re.sub("\.gz$", "", readset.fastq1)))
							fastq2 = os.path.join(input_dir, os.path.basename(re.sub("\.gz$", "", readset.fastq2)))
						else:
							# input fles are fastqs
							fastq1 = os.path.join(input_dir, os.path.basename(readset.fastq1))
							fastq2 = os.path.join(input_dir, os.path.basename(readset.fastq2))
					fastq1_list.append(fastq1)
					fastq2_list.append(fastq2)
				merge_fastq_job = merge_fastq.merge_fastq(fastq1_list, fastq2_list, input_dir)
				job = concat_jobs([
					merge_fastq_job,
				], name="merge_fastq." + sample.name)
				jobs.append(job)

		return jobs



	def fusionmap(self):
		"""
		Run FusionMap to call gene fusion
		"""
		jobs = []
		for sample in self.samples:
			fastq1, fastq2 = self.select_input_fastq(sample)
			out_dir = os.path.join("fusions", "fusionmap", sample.name)
			fusionmap_job = fusionmap.fusionmap(fastq1, fastq2, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				fusionmap_job,
				Job(command="ls " + out_dir + "/02_RNA*")
			
			], name="fusionmap." + sample.name)

			jobs.append(job)

		return jobs	
	def tophat2(self):
		"""
		Run Tophat2 for Integrate
		"""
		jobs = []
		for sample in self.samples:
			fastq1, fastq2 = self.select_input_fastq(sample)
			out_dir = os.path.join(self.output_dir, "fusions", "tophat2", sample.name)
			tophat2_job = tophat2.tophat2(fastq1, fastq2, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				tophat2_job
			], name="tophat2." + sample.name)

			jobs.append(job)

		return jobs	

	def integrate(self):
		"""
		Run Integrate to call gene fusion
		"""
		jobs = []
		for sample in self.samples:
			input_dir = os.path.join("fusions", "tophat2", sample.name)
			accepted_bam = os.path.join(self.output_dir, input_dir, "accepted_hits.bam")
			unmapped_bam = os.path.join(self.output_dir, input_dir, "unmapped.bam")

			out_dir = os.path.join("fusions", "integrate", sample.name)
			integrate_job = integrate.integrate(accepted_bam, unmapped_bam, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				Job(command="cd " + out_dir),
				integrate_job,
				Job(command="cd -")
			], name="integrate." + sample.name)

			jobs.append(job)

		return jobs	
	def integrate_make_result_file(self):
		"""
		Merge infomation from breakpoints.tsv and reads.txt
		"""
		jobs = []
		for sample in self.samples:
			input_dir = os.path.join("fusions", "integrate", sample.name)

			make_result_job = integrate.make_result_file(input_dir)
			job = concat_jobs([
				make_result_job
			], name="integrate_make_result." + sample.name)

			jobs.append(job)

		return jobs	
		
	def ericscript(self):
		"""
		Run EricScript to call gene fusion
		"""
		jobs = []
		for sample in self.samples:
			fastq1, fastq2 = self.select_input_fastq(sample)
			out_dir = os.path.join("fusions", "ericscript", sample.name)
			ericscript_job = ericscript.ericscript(fastq1, fastq2, out_dir)
			job = concat_jobs([
				Job(command="mkdir -p " + out_dir),
				Job(command="rm -r " + out_dir),
				ericscript_job
			], name="ericscript." + sample.name)

			jobs.append(job)

		return jobs	

	def convert_fusion_results_to_cff(self):
		"""
		Convert fusion results to cff format
		"""
		jobs = []
		out_dir = os.path.join("fusions", "cff")
		job_list = [Job(command="mkdir -p " + out_dir)]
		sampleinfo_file = os.path.relpath(self.args.sampleinfo.name, self.output_dir)
		for sample in self.samples:
			defuse_result = os.path.join("fusions", "defuse", sample.name, "results.filtered.tsv")
			fusionmap_result = os.path.join("fusions", "fusionmap", sample.name, "02_RNA.FusionReport.txt")
			ericscript_result = os.path.join("fusions", "ericscript", sample.name, "fusion.results.filtered.tsv")
			integrate_result = os.path.join("fusions", "integrate", sample.name, "breakpoints.cov.tsv")

			tool_results = [("defuse", defuse_result), ("fusionmap", fusionmap_result), ("ericscript", ericscript_result), ("integrate", integrate_result)]
			"""
			sample_type = ""
			for contrast in self.contrasts:
				if sample in contrast.controls:
					sample_type = "Normal"
				elif sample in contrast.treatments:
					sample_type = "Tumor"
				if sample_type:
					disease_name = contrast.name
					break	
			if not sample_type:
				raise Exception("Error: sample " + sample.name + " not found in design file " + self.args.design.name)
			"""	
			for tool, result_file in tool_results:
				job = cff_convertion.cff_convert(sample.name, result_file, sampleinfo_file, tool, out_dir)
				job.command = job.command.strip()
				job_list.append(job)
		job = concat_jobs(job_list, name="cff_convertion")
		jobs.append(job)
		return jobs

	def merge_and_reannotate_cff_fusion(self):
		"""
		Merge all cff files into one single file and reannotate it with given annotation files
		"""
		jobs = []
		cff_files = []
		cff_dir = os.path.join("fusions", "cff")
		out_dir = os.path.join("fusions", "cff")
		tool_list = ["defuse", "fusionmap", "ericscript", "integrate"]
		for tool in tool_list:
			cff_files.extend([os.path.join(cff_dir, sample.name+"."+tool+".cff") for sample in self.samples])
		
		reann_job = merge_and_reannotate_cff_fusion.merge_and_reannotate_cff_fusion(cff_files, out_dir)
		
		job = concat_jobs([
			reann_job	
		], name="merge_and_reannotate_cff_fusion")

		jobs.append(job)
		return jobs
		
	def check_dna_support_before_next_exon(self):
		"""
		Check DNA support (pair clusters) until the start of next exon/utr
		"""
		jobs = []
		dna_bam_list = os.path.abspath(self.args.dnabam.name)
		tmp_dir = os.path.join("fusions", "tmp")
		reann_file = os.path.join("fusions", "cff", "merged.cff.reann")
		dna_supp_job = check_dna_support_before_next_exon.check_dna_support_before_next_exon(reann_file, dna_bam_list, tmp_dir)
		job = concat_jobs([
			Job(command="mkdir -p " + tmp_dir),
			dna_supp_job	
		], name="check_dna_support_before_next_exon")

		jobs.append(job)
		return jobs
	
	def cluster_reann_dnasupp_file(self):
		"""
		Check DNA support (pair clusters) until the start of next exon/utr
		"""
		jobs = []
		out_dir = os.path.join("fusions", "cff")
		cluster_job = merge_and_reannotate_cff_fusion.cluster_reann_dnasupp_file(out_dir)
		
		job = concat_jobs([
			cluster_job	
		], name="cluster_reann_dnasupp_file")

		jobs.append(job)
		return jobs
		
	@property
	def steps(self):
		return [
			self.star_filter,
			self.defuse,
			self.fusionmap,
			self.ericscript,
			self.tophat2,
			self.integrate,
			self.integrate_make_result_file,
			self.convert_fusion_results_to_cff,
			self.merge_and_reannotate_cff_fusion,
			self.check_dna_support_before_next_exon,
			self.cluster_reann_dnasupp_file
		]

if __name__ == '__main__':
	RnaFusion()
